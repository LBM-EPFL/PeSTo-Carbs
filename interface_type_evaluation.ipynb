{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as pt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "from config import config_model, config_data\n",
    "from data_handler import Dataset\n",
    "from model import Model\n",
    "from src.dataset import collate_batch_features, select_by_sid, select_by_max_ba, select_by_interface_types\n",
    "from src.scoring import bc_scoring, bc_score_names\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Arial']\n",
    "rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataset(config_data, r_types_sel):\n",
    "    # set up dataset\n",
    "    dataset = Dataset(\"datasets/contacts_rr4A_64nn.h5\")\n",
    "\n",
    "    # selected structures\n",
    "    sids_sel = np.genfromtxt(\"datasets/subunits_validation_highres_set.txt\", dtype=np.dtype('U'))\n",
    "\n",
    "    # filter dataset\n",
    "    m = select_by_sid(dataset, sids_sel) # select by sids\n",
    "\n",
    "    # data selection criteria\n",
    "    m = select_by_sid(dataset, sids_sel) # select by sids\n",
    "    m &= select_by_max_ba(dataset, config_data['max_ba'])  # select by max assembly count\n",
    "    #m &= (dataset.sizes[:,0] <= config_data['max_size']) # select by max size\n",
    "    m &= (dataset.sizes[:,1] >= config_data['min_num_res'])  # select by min size\n",
    "    m &= select_by_interface_types(dataset, config_data['l_types'], np.concatenate(r_types_sel))  # select by interface type\n",
    "\n",
    "    # update dataset selection\n",
    "    dataset.update_mask(m)\n",
    "\n",
    "    # set dataset types\n",
    "    dataset.set_types(config_data['l_types'], config_data['r_types'])\n",
    "\n",
    "    # debug print\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataset, ids):\n",
    "    p_l, y_l = [], []\n",
    "    with pt.no_grad():\n",
    "        for i in tqdm(ids):\n",
    "            # get data\n",
    "            X, ids_topk, q, M, y = dataset[i]\n",
    "\n",
    "            # pack data and setup sink (IMPORTANT)\n",
    "            X, ids_topk, q, M = collate_batch_features([[X, ids_topk, q, M]])\n",
    "\n",
    "            # run model\n",
    "            z = model(X.to(device), ids_topk.to(device), q.to(device), M.float().to(device))\n",
    "\n",
    "            # prediction\n",
    "            p = pt.sigmoid(z)\n",
    "\n",
    "            # categorical predictions\n",
    "            pc = pt.cat([1.0 - pt.max(p, dim=1)[0].unsqueeze(1), p], dim=1).cpu()\n",
    "            yc = pt.cat([1.0 - pt.any(y > 0.5, dim=1).float().unsqueeze(1), y], dim=1).cpu()\n",
    "\n",
    "            # data\n",
    "            p_l.append(pc)\n",
    "            y_l.append(yc)\n",
    "\n",
    "    return p_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select saved model\n",
    "model_filepath = 'model_ckpt.pt'\n",
    "\n",
    "# define device\n",
    "device = pt.device(\"cuda\")\n",
    "\n",
    "# create model\n",
    "model = Model(config_model)\n",
    "\n",
    "# reload model\n",
    "model.load_state_dict(pt.load(model_filepath, map_location=pt.device(\"cuda\")))\n",
    "\n",
    "# set model to inference\n",
    "model = model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "p_l, y_l = [], []\n",
    "for i in range(len(config_data['r_types'])):\n",
    "    # debug print\n",
    "    print(config_data['r_types'][i])\n",
    "\n",
    "    # load datasets\n",
    "    dataset = setup_dataset(config_data, [config_data['r_types'][i]])\n",
    "    print(\"dataset: {}\".format(len(dataset)))\n",
    "\n",
    "    # parameters\n",
    "    N = min(len(dataset), 512)\n",
    "\n",
    "    # run negative examples\n",
    "    ids = np.arange(len(dataset))\n",
    "    np.random.shuffle(ids)\n",
    "    pi_l, yi_l = eval_model(model, dataset, ids[:N])\n",
    "\n",
    "    # store evaluation results\n",
    "    p_l.append(pi_l)\n",
    "    y_l.append(yi_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "class_names = [\"carbohydrates\", \"cyclodextrins\"]\n",
    "\n",
    "# compute scores per class\n",
    "scores = []\n",
    "for i in range(len(y_l)):\n",
    "    # extract class\n",
    "    p = pt.cat(p_l[i], dim=0)[:,i+1]\n",
    "    y = pt.cat(y_l[i], dim=0)[:,i+1]\n",
    "\n",
    "    # compute scores\n",
    "    s = bc_scoring(y.unsqueeze(1), p.unsqueeze(1)).squeeze().numpy()\n",
    "\n",
    "    # compute F1 score\n",
    "    f1 = metrics.f1_score(y.numpy().astype(int), p.numpy().round())\n",
    "\n",
    "    # compute ratio of positives\n",
    "    r = pt.mean(y)\n",
    "\n",
    "    # store results\n",
    "    scores.append(np.concatenate([s, [f1, r]]))\n",
    "\n",
    "# pack data\n",
    "scores = np.stack(scores).T\n",
    "\n",
    "# make table\n",
    "df = pd.DataFrame(data=np.round(scores,2), index=bc_score_names+['F1', 'r'], columns=class_names)\n",
    "\n",
    "# save dataframe\n",
    "df.to_csv(\"results/type_interface_search_scores.csv\")\n",
    "\n",
    "# display\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(5,4.5))\n",
    "for i in range(len(y_l)):\n",
    "    # get labels and predictions for class\n",
    "    yi = pt.cat(y_l[i], dim=0)[:,i+1]\n",
    "    pi = pt.cat(p_l[i], dim=0)[:,i+1]\n",
    "\n",
    "    # compute roc and roc auc\n",
    "    fpr, tpr, _ = metrics.roc_curve(yi.numpy(), pi.numpy())\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # update plot\n",
    "    plt.plot(fpr, tpr, '-', label=\"{} (auc: {:.2f})\".format(class_names[i], auc))\n",
    "\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/type_interface_search_roc_auc.svg\")\n",
    "plt.savefig(\"results/type_interface_search_roc_auc.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(5,4.5))\n",
    "for i in range(len(y_l)):\n",
    "    # get labels and predictions for class\n",
    "    yi = pt.cat(y_l[i], dim=0)[:,i+1]\n",
    "    pi = pt.cat(p_l[i], dim=0)[:,i+1]\n",
    "\n",
    "    # compute roc and roc auc\n",
    "    pre, rec, _ = metrics.precision_recall_curve(yi.numpy(), pi.numpy())\n",
    "    auc = metrics.auc(rec, pre)\n",
    "\n",
    "    # update plot\n",
    "    plt.plot(rec, pre, '-', label=\"{} (auc: {:.2f})\".format(class_names[i], auc))\n",
    "\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/type_interface_search_pr_auc.svg\")\n",
    "plt.savefig(\"results/type_interface_search_pr_auc.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack results\n",
    "P = pt.cat([pt.cat(p, dim=0) for p in p_l], dim=0)\n",
    "Y = pt.cat([pt.cat(y, dim=0) for y in y_l], dim=0)\n",
    "\n",
    "# select only residues at interface\n",
    "m = pt.any(Y[:,1:] > 0.5, dim=1)\n",
    "Pi = P[m,1:]\n",
    "Yi = Y[m,1:]\n",
    "\n",
    "# pick same sampling of all classes\n",
    "n = Yi.shape[1]\n",
    "N = int(pt.min(pt.sum(Yi, dim=0)).item())\n",
    "ids_unif = pt.from_numpy(np.concatenate([np.random.choice(pt.where(Yi[:,i] > 0.5)[0].numpy(), N, replace=False) for i in range(n)]))\n",
    "\n",
    "# compute scores for each class\n",
    "scores = []\n",
    "for i in range(n):\n",
    "    # extract class\n",
    "    p = Pi[ids_unif,i]\n",
    "    y = Yi[ids_unif,i]\n",
    "\n",
    "    # compute scores\n",
    "    s = bc_scoring(y.unsqueeze(1), p.unsqueeze(1)).squeeze().numpy()\n",
    "\n",
    "    # compute F1 score\n",
    "    f1 = metrics.f1_score(y.numpy().astype(int), p.numpy().round())\n",
    "\n",
    "    scores.append(np.concatenate([s, [f1]]))\n",
    "\n",
    "# pack data\n",
    "scores = np.stack(scores).T\n",
    "\n",
    "# make table\n",
    "df = pd.DataFrame(data=np.round(scores,2), index=bc_score_names+['F1'], columns=class_names)\n",
    "\n",
    "# save dataframe\n",
    "df.to_csv(\"results/type_interface_identification_scores.csv\")\n",
    "\n",
    "# display\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack results\n",
    "P = pt.cat([pt.cat(p, dim=0) for p in p_l], dim=0)\n",
    "Y = pt.cat([pt.cat(y, dim=0) for y in y_l], dim=0)\n",
    "\n",
    "# select only residues at interface\n",
    "m = pt.any(Y[:,1:] > 0.5, dim=1)\n",
    "Pi = P[m,1:]\n",
    "Yi = Y[m,1:]\n",
    "\n",
    "# pick same sampling of all classes\n",
    "n = Yi.shape[1]\n",
    "N = int(pt.min(pt.sum(Yi, dim=0)).item())\n",
    "ids_l = [pt.from_numpy(np.random.choice(pt.where(Yi[:,i] > 0.5)[0].numpy(), N, replace=False)) for i in range(n)]\n",
    "\n",
    "# compute scores for each class\n",
    "C = pt.zeros((n,n))\n",
    "for i in range(n):\n",
    "    ids = pt.argmax(Pi[ids_l[i]], dim=1)\n",
    "    for j,k in zip(ids, ids_l[i]):\n",
    "        C[i,j] += Pi[k,j].round()\n",
    "\n",
    "\n",
    "# normalize score\n",
    "H = (C / pt.sum(C, dim=1).unsqueeze(1)).numpy()\n",
    "#H = (C / pt.sum(C, dim=0).unsqueeze(0)).numpy()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(H, origin='lower', cmap='BuGn', vmin=0.0, vmax=1.0)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.xticks(np.arange(n), class_names, rotation=90)\n",
    "plt.yticks(np.arange(n), class_names)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        v = H[i,j]\n",
    "        if v > 0.1:\n",
    "            plt.text(j,i,f\"{v:.2f}\", ha='center', va='center', color=[np.round(v-0.1)]*3)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/type_interface_identification_most_confident_confusion_matrix.svg\")\n",
    "plt.savefig(\"results/type_interface_identification_most_confident_confusion_matrix.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack results\n",
    "P = pt.cat([pt.cat(p, dim=0) for p in p_l], dim=0)\n",
    "Y = pt.cat([pt.cat(y, dim=0) for y in y_l], dim=0)\n",
    "\n",
    "# select only residues at interface\n",
    "m = pt.any(Y[:,1:] > 0.5, dim=1)\n",
    "Pi = P[m,1:]\n",
    "Yi = Y[m,1:]\n",
    "\n",
    "# pick same sampling of all classes\n",
    "n = Yi.shape[1]\n",
    "N = int(pt.min(pt.sum(Yi, dim=0)).item())\n",
    "ids_l = [pt.from_numpy(np.random.choice(pt.where(Yi[:,i] > 0.5)[0].numpy(), N, replace=False)) for i in range(n)]\n",
    "\n",
    "# compute scores for each class\n",
    "C = pt.zeros((n,n))\n",
    "for i in range(n):\n",
    "    ids = pt.argmax(Pi[ids_l[i]], dim=1)\n",
    "    for j in range(n):\n",
    "        for k in ids_l[i]:\n",
    "            C[i,j] += Pi[k,j].round()\n",
    "\n",
    "\n",
    "# normalize score\n",
    "H = (C / pt.sum(C, dim=1).unsqueeze(1)).numpy()\n",
    "#H = (C / pt.sum(C, dim=0).unsqueeze(0)).numpy()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(H, origin='lower', cmap='BuGn', vmin=0.0, vmax=1.0)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.xticks(np.arange(n), class_names, rotation=90)\n",
    "plt.yticks(np.arange(n), class_names)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        v = H[i,j]\n",
    "        if v > 0.1:\n",
    "            plt.text(j,i,f\"{v:.2f}\", ha='center', va='center', color=[np.round(v-0.1)]*3)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/type_interface_identification_most_confident_confusion_matrix.svg\")\n",
    "plt.savefig(\"results/type_interface_identification_most_confident_confusion_matrix.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pesto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
